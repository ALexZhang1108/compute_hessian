{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96849832-20bb-4115-8fc7-374471d5ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0962650-bbc4-4acb-b0af-0e89d4f7390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3470200.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/train-images-idx3-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 101261.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/train-labels-idx1-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:01<00:00, 936071.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/t10k-images-idx3-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 6693790.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw/t10k-labels-idx1-ubyte.gz to /chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "    #cudnn.benchmark = True\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST的均值和标准差\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "kwargs = {'num_workers': 8, 'pin_memory': True}\n",
    "train_dataset = datasets.MNIST(root='/chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2', \n",
    "                              train=True, \n",
    "                              transform=transform,\n",
    "                              download=True)\n",
    "test_dataset = datasets.MNIST(root='/chenyupeng/old_files/yupeng_landscape/landscape_LLM/MNIST_MLP/data2', \n",
    "                             train=False, \n",
    "                             transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                         batch_size=60000, \n",
    "                         shuffle=False,**kwargs)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                        batch_size=60000, \n",
    "                        shuffle=False,**kwargs)\n",
    "input_size = 784  # 28x28\n",
    "hidden_size =  5\n",
    "output_size = 10  # 0-9的数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8fc3d7-e4bc-4e55-9fdf-462bd04076ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a755840-080f-4ff3-bee0-ecd12428bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size,bias=False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size,bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "862a574c-5fd2-4a20-8c20-30a0e14ed683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "eval_batches = train_loader # 使用前10个batch评估\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e000d2b1-9c3c-4db7-b61b-adec42e5a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(datalodaer, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        for images, labels in datalodaer:\n",
    "            labels = labels.cuda()\n",
    "            images = images.reshape(-1, 28 * 28).cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(datalodaer), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664b40de-a813-4bf2-86c5-8b3cbc5bb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_gradient(datalodaer, model):\n",
    "    model.eval()\n",
    "    #with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for images, labels in datalodaer:\n",
    "        labels = labels.cuda()\n",
    "        images = images.reshape(-1, 28 * 28).cuda()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        scaled_loss = loss / len(datalodaer)\n",
    "        scaled_loss.backward()  # 梯度会累积\n",
    "\n",
    "    \n",
    "    grad_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            #p.data -= lr * p.grad\n",
    "            grad_norm += (p.grad.norm())**2\n",
    "            p.grad = None  # 清空梯度\n",
    "            \n",
    "    grad_norm = torch.sqrt(grad_norm)\n",
    "    \n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e96b208-d7d1-4855-a11b-21b8f4cb53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_full_hessian(datalodaer, model):\n",
    "    model.eval()\n",
    "    model.zero_grad(set_to_none = True)\n",
    "    def hessian_calculation(g_tensor):\n",
    "        g_tensor = g_tensor.cuda()\n",
    "        total_params = g_tensor.size(0)\n",
    "        hessian_list = []\n",
    "        t_d = time.time()\n",
    "        for d in range(total_params):\n",
    "            unit_vector = torch.zeros(total_params)\n",
    "            unit_vector[d] = 1\n",
    "            unit_vector = unit_vector.cuda()\n",
    "            l = torch.sum(g_tensor * unit_vector)\n",
    "            l.backward(retain_graph= True)\n",
    "            hessian_row = []\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'ln' in name or 'bias' in name or 'wte' in name or 'wpe' in name:\n",
    "                    continue\n",
    "                if param.requires_grad:\n",
    "                    #print('name',name, param.grad)\n",
    "                    hessian_row.append(param.grad.double().data.clone())\n",
    "            \n",
    "            model.zero_grad(set_to_none = True)\n",
    "            hessian_row = [g.flatten() for g in hessian_row] \n",
    "            hessian_row = [g.cpu() for g in hessian_row]\n",
    "            hessian_row = torch.cat(hessian_row)\n",
    "            #print('hessian_row', hessian_row)   \n",
    "            hessian_list.append(hessian_row)\n",
    "            # if d % 1000 == 0:\n",
    "            #     print(f'Computing hessian: current batch = {batch_idx}/{self.num_batches}, current row of a hessian: {d}/{total_params}, total time = {time.time()- t_d} ')\n",
    "        hessian = torch.stack(hessian_list, dim = 1)\n",
    "        #print('hessian', hessian)   \n",
    "        return hessian\n",
    "    full_hessian = 0\n",
    "    for images, labels in datalodaer:\n",
    "        labels = labels.cuda()\n",
    "        images = images.reshape(-1, 28 * 28).cuda()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        scaled_loss = loss / len(datalodaer)\n",
    "        scaled_loss.backward(create_graph= True)\n",
    "        g_list = []\n",
    "        count = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            #if 'ln' in name or 'bias' in name:\n",
    "            if 'ln' in name or 'bias' in name or 'wte' in name or 'wpe' in name:\n",
    "                continue\n",
    "            if param.requires_grad:\n",
    "                count += param.numel()\n",
    "                #print('g shape', param.grad , param.grad.shape)\n",
    "                g_list.append(torch.flatten(param.grad.double()))\n",
    "                #print('name',name, g_list[-1].size())\n",
    "        g_tensor = torch.cat(g_list, dim = 0)\n",
    "\n",
    "        #print(g_tensor.shape)\n",
    "        #print('g_tensor',g_tensor)\n",
    "        model.zero_grad(set_to_none = True)\n",
    "        H = hessian_calculation(g_tensor)\n",
    "        full_hessian += H\n",
    "    full_hessian = torch.nan_to_num(full_hessian, nan = 0, posinf = 0, neginf = 0 )  # change nan, postive inf , negative inf, to 0\n",
    "    t_svd = time.time()\n",
    "    #print('doing EVD')\n",
    "    # _, eigenvalues, _ = torch.linalg.svd(full_hessian)  # ascending\n",
    "    #eigenvalues, _  = torch.eig(full_hessian)\n",
    "    full_hessian = full_hessian.numpy().astype(np.float64)\n",
    "    full_hessian = (full_hessian + full_hessian.T)/2 # make symetric, to \n",
    "    \n",
    "    \n",
    "    \n",
    "    #avoid numerical issue\n",
    "    #full_hessian = full_hessian.cuda()\n",
    "    #eigenvalues, _  = torch.linalg.eig(full_hessian)\n",
    "    # eigenvalues, _  = np.linalg.eigh(full_hessian)\n",
    "    # #_, eigenvalues, _ = np.linalg.svd(full_hessian) \n",
    "    # eigenvalues = [eigen.item().real for eigen in eigenvalues]\n",
    "    # file_name = self.file_dir + 'eigenvalues.txt'\n",
    "    # with open(file_name, \"w\") as file:\n",
    "    #     for item in eigenvalues:\n",
    "    #         file.write(str(item)+\"\\n\")\n",
    "    # print(f'EVD time = {time.time()- t_svd}')\n",
    "    return full_hessian, g_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "def09ed0-2983-4d18-9f3a-8b2351719646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "ckpt_dir= \"bs256_lr0.05_e30_hidden5_SGD_m0.9_seed0\"\n",
    "ckpt_name = [i*0.1 for i in range(1,200)]\n",
    "ckpt_files = [f\"{script_dir}/trained_MLP/{ckpt_dir}/model_para_e_{i:.1f}_decaying_2.0_pipecheck_w.pt\" for i in ckpt_name]\n",
    "\n",
    "all_decay_model = {}\n",
    "\n",
    "for name, ckpt in zip(ckpt_name, ckpt_files):\n",
    "    name_str = f\"{name:.1f}\"\n",
    "    temp_model = MLP(28*28, 5, 10)\n",
    "    state_dict = torch.load(ckpt, map_location=torch.device('cpu'))\n",
    "        ## 兼容 state_dict 可能存在 'model' key 的情况\n",
    "        #if 'model_state_dict' in state_dict:\n",
    "    temp_model.load_state_dict(state_dict['state_dict'])\n",
    "    all_decay_model.update({name_str: temp_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a6693b2-f37e-4ae1-b674-3ec460d33186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1177.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "hessian_0, grad = get_full_hessian(eval_batches, all_decay_model['0.1'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f0fbe4-b90f-4863-8d21-903a76cc1e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3970])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea55dee-0cb0-43a5-bebb-bb8bbacaee08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=784, out_features=5, bias=False)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb83efbf-22cf-429a-920f-4fb57b9c3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hessian_0 = torch.tensor(hessian_0)\n",
    "\n",
    "u,sigma,d = torch.svd(hessian_0.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65bac937-73bb-4d13-ae9d-5b4e429e2f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7800e+01, 1.4120e+01, 1.2046e+01,  ..., 5.9897e-37, 3.8653e-37,\n",
       "        3.7000e-37], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aff11ecd-26a8-4eab-bb4a-247a3cb55bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.0027]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(grad.reshape(-1,1).T, torch.matmul(hessian_0.cuda(), grad.reshape(-1,1)))/torch.norm(grad)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab3a5fe-653d-45bb-9f1b-499958fa839c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "082c8a4b-b87e-4968-a14a-381c7a03bbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.9624]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian_0, grad = get_full_hessian(eval_batches, all_decay_model['1.0'].cuda())\n",
    "hessian_0 = torch.tensor(hessian_0)\n",
    "\n",
    "u,sigma,d = torch.svd(hessian_0.cuda())\n",
    "torch.matmul(grad.reshape(-1,1).T, torch.matmul(hessian_0.cuda(), grad.reshape(-1,1)))/torch.norm(grad)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8503f010-e297-4b0f-9ed9-ad26db4a3153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.7162]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian_0, grad = get_full_hessian(eval_batches, all_decay_model['19.0'].cuda())\n",
    "hessian_0 = torch.tensor(hessian_0)\n",
    "\n",
    "u,sigma,d = torch.svd(hessian_0.cuda())\n",
    "torch.matmul(grad.reshape(-1,1).T, torch.matmul(hessian_0.cuda(), grad.reshape(-1,1)))/torch.norm(grad)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534efc51-ad31-459d-bb16-ef9799a73328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm : 0.16438806056976318\n",
      "gradient norm : 0.1482352763414383\n",
      "gradient norm : 0.1415596306324005\n",
      "gradient norm : 0.13238200545310974\n",
      "gradient norm : 0.11943792551755905\n",
      "gradient norm : 0.11839990317821503\n",
      "gradient norm : 0.11563529819250107\n",
      "gradient norm : 0.11019901931285858\n",
      "gradient norm : 0.10939928889274597\n",
      "gradient norm : 0.1074424460530281\n",
      "gradient norm : 0.10870210826396942\n",
      "gradient norm : 0.10800721496343613\n",
      "gradient norm : 0.10809173434972763\n",
      "gradient norm : 0.10720718652009964\n",
      "gradient norm : 0.11178351938724518\n",
      "gradient norm : 0.10957272350788116\n",
      "gradient norm : 0.10929979383945465\n",
      "gradient norm : 0.10949341952800751\n",
      "gradient norm : 0.10991955548524857\n",
      "gradient norm : 0.1070091649889946\n",
      "gradient norm : 0.11140109598636627\n",
      "gradient norm : 0.11072598397731781\n",
      "gradient norm : 0.11037047207355499\n",
      "gradient norm : 0.10863151401281357\n",
      "gradient norm : 0.10888320952653885\n",
      "gradient norm : 0.11008898913860321\n",
      "gradient norm : 0.1091446802020073\n",
      "gradient norm : 0.1129290834069252\n",
      "gradient norm : 0.11301445215940475\n",
      "gradient norm : 0.11280441284179688\n",
      "gradient norm : 0.11285840719938278\n",
      "gradient norm : 0.11119568347930908\n",
      "gradient norm : 0.11221247166395187\n",
      "gradient norm : 0.11306270211935043\n",
      "gradient norm : 0.11401087045669556\n",
      "gradient norm : 0.11179818212985992\n",
      "gradient norm : 0.1104220449924469\n",
      "gradient norm : 0.11000929772853851\n",
      "gradient norm : 0.10993611067533493\n",
      "gradient norm : 0.11041568219661713\n",
      "gradient norm : 0.11044498533010483\n",
      "gradient norm : 0.10865114629268646\n",
      "gradient norm : 0.10913403332233429\n",
      "gradient norm : 0.11039356142282486\n",
      "gradient norm : 0.11005805432796478\n",
      "gradient norm : 0.10864916443824768\n",
      "gradient norm : 0.10911142081022263\n",
      "gradient norm : 0.10909807682037354\n",
      "gradient norm : 0.10789350420236588\n",
      "gradient norm : 0.10755917429924011\n",
      "gradient norm : 0.10850795358419418\n",
      "gradient norm : 0.10834056884050369\n"
     ]
    }
   ],
   "source": [
    "for v in all_decay_model.values():\n",
    "    print(f\"gradient norm : {eval_gradient(eval_batches, v.cuda())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5ab91-f6d4-49f7-8d7b-ec977ea62910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
